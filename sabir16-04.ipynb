{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import tokenize, stem\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "from unidecode import unidecode\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(tweets):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    tweets - List of strings, the strings being tweets.\n",
    "    -----------------------------------------------------------------\n",
    "    Return-value:\n",
    "    \n",
    "    tokens - 2-dimensional list containing tokens as strings.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = list()\n",
    "    tk = tokenize.TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            element = tk.tokenize(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            element = []\n",
    "        tokens.append(element)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    tokens - List of lists containing tokens of the tweets.\n",
    "    -----------------------------------------------------------------\n",
    "    Return-value:\n",
    "    \n",
    "    final_tokens - List of lists containing normalized tokens of the tweets.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    exclude = set(string.punctuation)\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    \n",
    "    # Unicode to string\n",
    "    tokens_str = [[unidecode(token) for token in tweet] for tweet in tokens]\n",
    "    \n",
    "    #Replacing URLs with empty string\n",
    "    tokens_str = [[sub(r\"http\\S+\", \"\", token) for token in tweet] for tweet in tokens_str]\n",
    "    \n",
    "    # Removing punctuation    \n",
    "    tokens_str = [[''.join(ch for ch in token if ch not in exclude) for token in tweet] for tweet in tokens_str]\n",
    "    \n",
    "    # Removing numbers\n",
    "    tokens_str = [[sub(r'\\d+', '', token) for token in tweet] for tweet in tokens_str]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens_str = [[stemmer.stem(token) for token in tweet] for tweet in tokens_str]\n",
    "    \n",
    "    # Unicode to string\n",
    "    tokens_str = [[unidecode(token) for token in tweet] for tweet in tokens_str]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    tokens_str = [[word for word in tweet if word not in stop] for tweet in tokens_str]\n",
    "    \n",
    "    # Removing empty tokens (strings)\n",
    "    final_tokens = list()\n",
    "    for str_list in tokens_str:\n",
    "        x = filter(None, str_list)\n",
    "        final_tokens.append(x)\n",
    "        \n",
    "    # Removing periods in abbreviations. Ex: U.S.A. to USA\n",
    "\n",
    "    # for tweet in tokens:\n",
    "    #     tweet = [sub(r'(?<!\\w)([A-Z])\\.', r'\\1', x.lower()) for x in tweet]\n",
    "    #     print (tweet)\n",
    "    \n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def createInvertedIndex(tokens):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    tokens - List of lists containing normalized tokens.\n",
    "    ------------------------------------------------------------------\n",
    "    Return-value:\n",
    "    \n",
    "    inverted_index - Dictionary containing the index words.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inverted_index = {}\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if tokens[i][j]:\n",
    "                if inverted_index.has_key(tokens[i][j]):\n",
    "                    if i not in inverted_index[tokens[i][j]]:\n",
    "                        inverted_index[tokens[i][j]].append(i)\n",
    "                else:\n",
    "                    inverted_index[tokens[i][j]] = [i]\n",
    "#         print(i)\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def bm25(query_tokens, index, tokens):\n",
    "    \n",
    "    map_id_score = {}\n",
    "    k1 = 1.2\n",
    "    b = 0.75 \n",
    "    N = len(tokens) # Number of documents in the collection\n",
    "    adder = 0\n",
    "    \n",
    "    for doc_tokens in tokens:\n",
    "        adder += len(doc_tokens)\n",
    "    \n",
    "    avg_doclen = float(adder)/N\n",
    "    \n",
    "    for j in range(len(tokens)):\n",
    "        \n",
    "        lend = float(len(tokens[j]))\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(query_tokens)):\n",
    "            \n",
    "            if query_tokens[i] in tokens[j]:\n",
    "                \n",
    "                n = float(len(index[query_tokens[i]]))\n",
    "                f = float(tokens[j].count(query_tokens[i]))\n",
    "                T1 = math.log(float(N-n+0.5)/(n+0.5),2)\n",
    "                x = k1 * ((1-b) + b*(lend/avg_doclen)) + f\n",
    "                T2 = float((k1+1)*f)/x\n",
    "                score += T1*T2\n",
    "        \n",
    "        map_id_score[j] = score\n",
    "        \n",
    "    return map_id_score\n",
    "\n",
    "def generateFeatures(i, sorted_scores, threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    i - ID of the tweet in the test dataset\n",
    "    sorted_scores - List of tuples containing TweetID and Similarity score, sorted in descending order of scores\n",
    "    threshold - Number of results to retrieve from the ranking generated.\n",
    "    -------------------------------------------------------------------------------------------------------------\n",
    "    Return-value:\n",
    "    \n",
    "    features - Dictionary containing 24 features\n",
    "    \n",
    "    \"\"\"\n",
    "    global map_id_sentiment\n",
    "    \n",
    "    # Features dictionary\n",
    "    features = {\n",
    "        'avg_n' : 0.0,\n",
    "        'max_n' : 0.0,\n",
    "        'min_n' : 0.0,\n",
    "        'sum_n' : 0.0,\n",
    "        'count_n' : 0.0,\n",
    "        'phi_n' : 0.0,\n",
    "        'phi_pos_n' : 0.0,\n",
    "        'phi_avg_n' : 0.0,\n",
    "        'phi_max_n' : 0.0,\n",
    "        'phi_min_n' : 0.0,\n",
    "        'phi_sum_n' : 0.0,\n",
    "        'phi_count_n' : 0.0,\n",
    "        'avg_p' : 0.0,\n",
    "        'max_p' : 0.0,\n",
    "        'min_p' : 0.0,\n",
    "        'sum_p' : 0.0,\n",
    "        'count_p' : 0.0,\n",
    "        'phi_p' : 0.0,\n",
    "        'phi_pos_p' : 0.0,\n",
    "        'phi_avg_p' : 0.0,\n",
    "        'phi_max_p' : 0.0,\n",
    "        'phi_min_p' : 0.0,\n",
    "        'phi_sum_p' : 0.0,\n",
    "        'phi_count_p' : 0.0 \n",
    "    }\n",
    "    \n",
    "    # Calculating aggregation functions feature values\n",
    "    for i in range(0, threshold):\n",
    "        if map_id_sentiment[sorted_scores[i][0]] == 0:\n",
    "            features['count_n'] += 1\n",
    "            features['sum_n'] += sorted_scores[i][1]\n",
    "            if features['count_n'] == 1.0:\n",
    "                features['max_n'] = sorted_scores[i][1]\n",
    "                features['min_n'] = sorted_scores[i][1]\n",
    "            elif sorted_scores[i][1] < features['min_n']:\n",
    "                features['min_n'] = sorted_scores[i][1]\n",
    "        else:\n",
    "            features['count_p'] += 1\n",
    "            features['sum_p'] += sorted_scores[i][1]\n",
    "            if features['count_p'] == 1.0:\n",
    "                features['max_p'] = sorted_scores[i][1]\n",
    "                features['min_p'] = sorted_scores[i][1]\n",
    "            elif sorted_scores[i][1] < features['min_p']:\n",
    "                features['min_p'] = sorted_scores[i][1]\n",
    "    \n",
    "    if features['count_n'] > 0.0:\n",
    "        features['avg_n'] = features['sum_n']/features['count_n']\n",
    "    if features['count_p'] > 0.0:\n",
    "        features['avg_p'] = features['sum_p']/features['count_p']\n",
    "    \n",
    "    # Calculating phi feature values\n",
    "    \n",
    "    rank_rel_n = 0\n",
    "    rank_rel_p = 0\n",
    "    \n",
    "    for i in range(0, threshold):\n",
    "        if map_id_sentiment[sorted_scores[i][0]] == 0:\n",
    "            rank_rel_n += 1\n",
    "            features['phi_n'] += float(rank_rel_n)/(i+1)\n",
    "            features['phi_pos_n'] += (float(rank_rel_n)/(i+1)) * sorted_scores[i][1]\n",
    "        else:\n",
    "            rank_rel_p += 1\n",
    "            features['phi_p'] += float(rank_rel_p)/(i+1)\n",
    "            features['phi_pos_p'] += (float(rank_rel_p)/(i+1)) * sorted_scores[i][1]\n",
    "    \n",
    "    if features['avg_n'] > 0.0:\n",
    "        features['phi_avg_n'] = features['phi_n']/features['avg_n']\n",
    "    if features['max_n'] > 0.0:\n",
    "        features['phi_max_n'] = features['phi_n']/features['max_n']\n",
    "    if features['min_n'] > 0.0:\n",
    "        features['phi_min_n'] = features['phi_n']/features['min_n']\n",
    "    if features['count_n'] > 0.0:\n",
    "        features['phi_count_n'] = features['phi_n']/features['count_n']\n",
    "    if features['sum_n'] > 0.0:\n",
    "        features['phi_sum_n'] = features['phi_n']/features['sum_n']\n",
    "    \n",
    "    if features['avg_p'] > 0.0:\n",
    "        features['phi_avg_p'] = features['phi_p']/features['avg_p']\n",
    "    if features['max_p'] > 0.0:\n",
    "        features['phi_max_p'] = features['phi_p']/features['max_p']\n",
    "    if features['min_p'] > 0.0:\n",
    "        features['phi_min_p'] = features['phi_p']/features['min_p']\n",
    "    if features['count_p'] > 0.0:\n",
    "        features['phi_count_p'] = features['phi_p']/features['count_p']\n",
    "    if features['sum_p'] > 0.0:\n",
    "        features['phi_sum_p'] = features['phi_p']/features['sum_p']\n",
    "    \n",
    "    return features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training and testing dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading training and testing dataset...\")\n",
    "\n",
    "df_full = pd.read_csv('stanford/training.1600000.processed.noemoticon.csv', header=None)\n",
    "df_testdata = pd.read_csv('stanford/testdata.manual.2009.06.14.csv', header=None)\n",
    "df_test = df_testdata.loc[df_testdata[0].isin([0,4])]\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df_full.iloc[:30000]\n",
    "df2 = df_full.iloc[800000:830000]\n",
    "df = pd.concat([df1,df2])\n",
    "tweets = list(df[5])\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tweetIDs to tweet and sentiment...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapping tweetIDs to tweet and sentiment...\")\n",
    "\n",
    "map_id_tweet = {}\n",
    "map_id_sentiment = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    map_id_tweet[i] = df.iloc[i][5]\n",
    "    map_id_sentiment[i] = df.iloc[i][0]\n",
    "\n",
    "# for i in range(len(df2)):\n",
    "#     map_id_tweet[i] = df2.iloc[i][5]\n",
    "#     map_id_sentiment[i] = df2.iloc[i][0]\n",
    "\n",
    "print(\"Done.\")\n",
    "# pool = Pool()\n",
    "# tokens = pool.map(tokenizer, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "Done.\n",
      "[[u'http://twitpic.com/2y1zl', u'-', u'awww', u',', u\"that's\", u'a', u'bummer', u'.', u'you', u'shoulda', u'got', u'david', u'carr', u'of', u'third', u'day', u'to', u'do', u'it', u'.', u';D'], [u'is', u'upset', u'that', u'he', u\"can't\", u'update', u'his', u'facebook', u'by', u'texting', u'it', u'...', u'and', u'might', u'cry', u'as', u'a', u'result', u'school', u'today', u'also', u'.', u'blah', u'!'], [u'i', u'dived', u'many', u'times', u'for', u'the', u'ball', u'.', u'managed', u'to', u'save', u'50', u'%', u'the', u'rest', u'go', u'out', u'of', u'bounds']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the tweeets\n",
    "print(\"Tokenizing tweets...\")\n",
    "tokenized_tokens = tokenizer(tweets)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(tokenized_tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing tweets...\n",
      "Done.\n",
      "[['awww', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day', 'D'], ['upset', 'cant', 'updat', 'hi', 'facebook', 'text', 'might', 'cri', 'result', 'school', 'today', 'also', 'blah'], ['dive', 'mani', 'time', 'ball', 'manag', 'save', 'rest', 'go', 'bound']]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the tweets\n",
    "print(\"Normalizing tweets...\")\n",
    "normalized_tokens = normalize(tokenized_tokens)\n",
    "print(\"Done.\")\n",
    "print(normalized_tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Inverted Index...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Creating Inverted Index\n",
    "print(\"Creating Inverted Index...\")\n",
    "index = createInvertedIndex(normalized_tokens)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test queries...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing test queries...\")\n",
    "queries = list(df_test[5])\n",
    "tokens1 = tokenizer(queries)\n",
    "query_tokens = normalize(tokens1)\n",
    "# print(query_tokens)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of tweets in the corpus: 60000\n",
      "Number of index terms: 29624\n",
      "\n",
      "Printing the top 10 results for each query:\n",
      "\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of tweets in the corpus: \" + str(len(tweets)))\n",
    "print(\"Number of index terms: \" + str(len(index)))\n",
    "threshold = 25\n",
    "features_list = list()\n",
    "print(\"\\nPrinting the top 10 results for each query:\\n\\n\")\n",
    "\n",
    "for i in range(0,len(queries)):\n",
    "    map_id_score = bm25(query_tokens[i], index, normalized_tokens)\n",
    "    data = map_id_score.items()\n",
    "    sorted_scores = sorted(data, key=lambda x: x[1],reverse=True)\n",
    "    query_features = generateFeatures(i, sorted_scores, threshold)\n",
    "    features_list.append(query_features)\n",
    "    print(i)\n",
    "#     print(\"\\n\\nQuery : \" + str(queries[i]))\n",
    "#     print(\"\\nMost Relevant Results : \\n\")\n",
    "#     for j in range(threshold):\n",
    "#         print(\"[\" + str(j+1) + \"] : (Score = \" + str(round(sorted_scores[j][1],4)) + \") \" + df.iloc[sorted_scores[j][0]][5])\n",
    "\n",
    "features_df = pd.DataFrame(features_list)\n",
    "# print(features_df)\n",
    "features_df.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
